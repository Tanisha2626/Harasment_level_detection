{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanisha2626/Harasment_level_detection/blob/main/Harasment_Level_detection_Bengali_(2)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZBrSpdk4S5w"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install /content/transformers/\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmMKCJ-dG2ge"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBSxMIY36Gdp"
      },
      "source": [
        "### TPU set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk2rAvu3y5Vk"
      },
      "source": [
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFCqNsgb6ARO"
      },
      "source": [
        "### Downloading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apm76bwvBo7d"
      },
      "source": [
        "!git clone https://github.com/Tanisha2626/Harasment_level_detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKKJ-JmZG2g3"
      },
      "source": [
        "raw_train_df =  pd.read_csv(\"Harasment_level_detection/trac2_iben_train.csv\")\n",
        "raw_train_df['split'] = 'train'\n",
        "print(raw_train_df.columns)\n",
        "print(raw_train_df['Sub-task A'].value_counts())\n",
        "print(raw_train_df['Sub-task B'].value_counts())\n",
        "print(f\"Size of 'train' split: {len(raw_train_df)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSM9RGfRG2g8"
      },
      "source": [
        "raw_dev_df =  pd.read_csv(\"Harasment_level_detection/trac2_iben_dev.csv\")\n",
        "raw_dev_df['split'] = 'dev'\n",
        "print(raw_dev_df.columns)\n",
        "print(raw_dev_df['Sub-task A'].value_counts())\n",
        "print(raw_dev_df['Sub-task B'].value_counts())\n",
        "print(f\"Size of 'dev' split: {len(raw_dev_df)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2TUSO4ZJNvM"
      },
      "source": [
        "test_df =  pd.read_csv(\"Harasment_level_detection/trac2_iben_test.csv\")\n",
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W99Hc1Y-G2hA"
      },
      "source": [
        "# Concatinate both train and dev dfs together\n",
        "data_df = pd.concat([raw_dev_df, raw_train_df], ignore_index= True)\n",
        "data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z1dLUbuG2hD"
      },
      "source": [
        "### Samples given per label size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad1zsEMPG2hE"
      },
      "source": [
        "print(f'Total dev + train size = {len(data_df)}\\n')\n",
        "print(data_df['Sub-task A'].value_counts(),'\\n')\n",
        "print(data_df['Sub-task B'].value_counts(),'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hY0WnQuG2hH"
      },
      "source": [
        "### Label encoder for Sub-task A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdJz_1d_bhV1"
      },
      "source": [
        "task_a_label_dict = {'NAG':0, 'OAG':1, 'CAG':2}\n",
        "print(task_a_label_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhws_lcgbYwy"
      },
      "source": [
        "data_df_task_a = data_df[['ID','Text','Sub-task A','split']].copy()\n",
        "data_df_task_a.columns.values[1] = 'text'\n",
        "data_df_task_a.columns.values[2] = 'label'\n",
        "data_df_task_a.loc[:,'label'] = data_df_task_a.loc[:,'label'].map(task_a_label_dict)\n",
        "data_df_task_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPAImYUnb21p"
      },
      "source": [
        "print(\"Num samples per class\")\n",
        "print(data_df_task_a.label.value_counts())\n",
        "\n",
        "print(\"\\nNum samples per split\")\n",
        "print(data_df_task_a.split.value_counts())\n",
        "\n",
        "print(\"\\nLabel counts in dev split\")\n",
        "print(data_df_task_a[data_df_task_a.split=='dev'].label.value_counts())\n",
        "\n",
        "print(\"\\nLabel counts in train split\")\n",
        "print(data_df_task_a[data_df_task_a.split=='train'].label.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O7vBHfC5iJd"
      },
      "source": [
        "### Label encoder for Sub-task B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqQas0q9G2hI",
        "scrolled": true
      },
      "source": [
        "task_b_label_dict = {'NGEN':0, 'GEN':1}\n",
        "print(task_b_label_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nzmWDI7G2hN"
      },
      "source": [
        "data_df_task_b = data_df[['ID','Text','Sub-task B','split']].copy()\n",
        "data_df_task_b.columns.values[1] = 'text'\n",
        "data_df_task_b.columns.values[2] = 'label'\n",
        "data_df_task_b.loc[:,'label'] = data_df_task_b.loc[:,'label'].map(task_b_label_dict)\n",
        "data_df_task_b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQxb1PR7G2hR"
      },
      "source": [
        "print(\"Num samples per class\")\n",
        "print(data_df_task_b.label.value_counts())\n",
        "\n",
        "print(\"\\nNum samples per split\")\n",
        "print(data_df_task_b.split.value_counts())\n",
        "\n",
        "print(\"\\nLabel counts in dev split\")\n",
        "print(data_df_task_b[data_df_task_b.split=='dev'].label.value_counts())\n",
        "\n",
        "print(\"\\nLabel counts in train split\")\n",
        "print(data_df_task_b[data_df_task_b.split=='train'].label.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri9x5Ksi5RL0"
      },
      "source": [
        "# Sub-task B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zmc7uQXkDP2l"
      },
      "source": [
        "#gb = data_df_task_a.groupby('split')\n",
        "gb = data_df_task_b.groupby('split')\n",
        "grps = [gb.get_group(x) for x in gb.groups]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL4pCY1uDp4I"
      },
      "source": [
        "grps[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV0mKnVk6Or6"
      },
      "source": [
        "### Model for sub-task b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuW0k9DRQyhb"
      },
      "source": [
        "def build_model_task_b(transformer, num_classes, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    #cls_token = sequence_output[:, 0, :]\n",
        "    #out = Dense(num_classes, activation='softmax')(cls_token)\n",
        "    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1))(sequence_output)\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "    X = tf.keras.layers.Dense(50, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dropout(0.2)(X)\n",
        "    out = tf.keras.layers.Dense(num_classes, activation='softmax')(X)\n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    #model.layers[1].trainable = False\n",
        "    model.compile(Adam(lr=1e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAq_gLJ_6j2s"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CtVao6pQ12w"
      },
      "source": [
        "EPOCHS = 2\n",
        "BATCH_SIZE = 16 #* strategy.num_replicas_in_sync\n",
        "MAX_LEN = 128\n",
        "MODEL = 'jplu/tf-xlm-roberta-base'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXmQP8s6op1"
      },
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV_zsFzJRbsY"
      },
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True, add_special_tokens=True,max_length=512, pad_to_max_length=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_lcemyWmFjR"
      },
      "source": [
        "x_train = np.asarray(tokenizer.batch_encode_plus(grps[1]['text'],pad_to_max_length=True, max_length=128, return_attention_mask=False)['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ91yI_Imoi3"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zicspfp9qDv1"
      },
      "source": [
        "y_train = grps[1].label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmNdbJT0fVSR"
      },
      "source": [
        "y_train = np.asarray(y_train, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaDw4_-MD95p"
      },
      "source": [
        "x_val = np.asarray(tokenizer.batch_encode_plus(grps[0]['text'],pad_to_max_length=True, max_length=128, return_attention_mask=False)['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F01gejQIEEis"
      },
      "source": [
        "x_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o17I-VgnEJnC"
      },
      "source": [
        "y_val = grps[0].label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCMYfuNdEOcV"
      },
      "source": [
        "y_val = np.asarray(y_val, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fy3nywDKvO_"
      },
      "source": [
        "x_test = np.asarray(tokenizer.batch_encode_plus(test_df['Text'],pad_to_max_length=True, max_length=128, return_attention_mask=False)['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-ALznf1X1pu"
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bflCMarEaMtz"
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "val_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_val, y_val))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_QPbHxT6837"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K64jMyljd3Ju"
      },
      "source": [
        "# # Train on CPU/GPU\n",
        "\n",
        "# transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "# model_task_b = build_model_task_b(transformer_layer,num_classes=2, max_len=MAX_LEN)\n",
        "# model_task_b.summary()\n",
        "\n",
        "# n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "# train_history = model_task_b.fit(\n",
        "#     train_dataset,\n",
        "#     steps_per_epoch=n_steps,\n",
        "#     validation_data=val_dataset,\n",
        "#     epochs=EPOCHS\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLFpHHhRi57q"
      },
      "source": [
        "# Train on TPU\n",
        "\n",
        "with strategy.scope():\n",
        "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "    model_task_b = build_model_task_b(transformer_layer,num_classes=2, max_len=MAX_LEN)\n",
        "model_task_b.summary()\n",
        "\n",
        "n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "train_history = model_task_b.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgie--igHCvp"
      },
      "source": [
        "np.argmax(model_task_b.predict(x_test),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWkm5_q-xyo3"
      },
      "source": [
        "predicted= np.argmax(model_task_b.predict(x_val),axis=1)\n",
        "\n",
        "y_true = np.asarray(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X82i7aVxzFp"
      },
      "source": [
        "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,confusion_matrix\n",
        "\n",
        "print(precision_score(y_true,predicted,average='macro'))\n",
        "print(recall_score(y_true,predicted,average='macro'))\n",
        "print(f1_score(y_true,predicted,average='macro'))\n",
        "print(accuracy_score(y_true,predicted))\n",
        "print(confusion_matrix(y_true, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKnXRfQB7AbT"
      },
      "source": [
        "## Testing function for sub-task b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRF2yay5cf2P",
        "cellView": "code"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def gender_bias(sentence):\n",
        "  text = [sentence]\n",
        "  df_input = pd.DataFrame(text)\n",
        "  x_input = np.asarray(tokenizer.batch_encode_plus(df_input[0],pad_to_max_length=True, max_length=128, return_attention_mask=False)['input_ids'])\n",
        "  class_probs = model_task_b.predict(x_input)\n",
        "  pred_class = np.argmax(class_probs,axis=1)[0]\n",
        "  gender = list(task_b_label_dict.keys())[pred_class]\n",
        "  return gender, class_probs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Ol2kjM7FAb"
      },
      "source": [
        "## GUI input for sub-task b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUZYLX09dCo5",
        "cellView": "form",
        "outputId": "3d2b06f8-8f6b-4939-aaad-143aae7c91ac"
      },
      "source": [
        "#@title Is this comment gender targeted?\n",
        "sentence = \"tumi paagol mohila\" #@param {type:\"string\"}\n",
        "gender, class_probs = gender_bias(sentence)\n",
        "print(\"Is it Gender Targeted ? = \",gender)\n",
        "print(\"Not Gender Targeted probability =\",class_probs[0])\n",
        "print(\"Gender Targeted probability =\",class_probs[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is it Gender Targeted ? =  GEN\n",
            "Not Gender Targeted probability = 0.38148755\n",
            "Gender Targeted probability = 0.6185124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04TIXlGxsIwd"
      },
      "source": [
        "# Sub-Task-A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlpfTszCh6JH"
      },
      "source": [
        "gb = data_df_task_a.groupby('split')\n",
        "grps = [gb.get_group(x) for x in gb.groups]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIr7Lyvqh6JI"
      },
      "source": [
        "x_train = np.asarray(tokenizer.batch_encode_plus(grps[1]['text'],pad_to_max_length=True, max_length=128, return_attention_mask=False)['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_0VeOfXh6JJ"
      },
      "source": [
        "y_train = grps[1].label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uibp6U2Kh6JM"
      },
      "source": [
        "y_train = np.asarray(y_train, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wtBUcOWh6JM"
      },
      "source": [
        "x_val = np.asarray(tokenizer.batch_encode_plus(grps[0]['text'],pad_to_max_length=True, max_length=128, return_attention_mask=False)['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysHYMd4Jh6JM"
      },
      "source": [
        "y_val = grps[0].label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_46kDJ3dh6JM"
      },
      "source": [
        "y_val = np.asarray(y_val, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKMYPfwah6JO"
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "val_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_val, y_val))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jh1ZiD27J9c"
      },
      "source": [
        "## Model for sub-task A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnSqJuDYr2_D"
      },
      "source": [
        "def build_model_task_a(transformer, num_classes, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    #cls_token = sequence_output[:, 0, :]\n",
        "    #out = Dense(num_classes, activation='softmax')(cls_token)\n",
        "    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1))(sequence_output)\n",
        "    X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "    X = tf.keras.layers.Dense(50, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dropout(0.2)(X)\n",
        "    out = tf.keras.layers.Dense(num_classes, activation='softmax')(X)\n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    #model.layers[1].trainable = False\n",
        "    model.compile(Adam(lr=1e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOVqcSWZ7QH_"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCbP-ol8xdvb"
      },
      "source": [
        "# # Train on CPU/GPU\n",
        "\n",
        "# transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "# model_task_a = build_model_task_a(transformer_layer,num_classes=3, max_len=MAX_LEN)\n",
        "# model_task_a.summary()\n",
        "\n",
        "# n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "# train_history = model_task_a.fit(\n",
        "#     train_dataset,\n",
        "#     steps_per_epoch=n_steps,\n",
        "#     validation_data=val_dataset,\n",
        "#     epochs=20\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6bwfYJ-h6JO"
      },
      "source": [
        "# Train on TPU\n",
        "with strategy.scope():\n",
        "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "    model_task_a = build_model_task_a(transformer_layer, num_classes=3, max_len=MAX_LEN)\n",
        "model_task_a.summary()\n",
        "\n",
        "n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "train_history = model_task_a.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTGoYS3R7TCH"
      },
      "source": [
        "predicted= np.argmax(model_task_a.predict(x_val),axis=1)\n",
        "\n",
        "y_true = np.asarray(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gOnF8j57SzK"
      },
      "source": [
        "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,confusion_matrix\n",
        "\n",
        "print(precision_score(y_true,predicted,average='macro'))\n",
        "print(recall_score(y_true,predicted,average='macro'))\n",
        "print(f1_score(y_true,predicted,average='macro'))\n",
        "print(accuracy_score(y_true,predicted))\n",
        "print(confusion_matrix(y_true, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHld0sQh7TNp"
      },
      "source": [
        "## Testing function for sub-task A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZo3QFpeiKSE"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def agression(sentence):\n",
        "  text = [sentence]\n",
        "  df_input = pd.DataFrame(text)\n",
        "  x_input = np.asarray(tokenizer.batch_encode_plus(df_input[0],pad_to_max_length=True, max_length=128, return_attention_mask=False)['input_ids'])\n",
        "  class_probs = model_task_a.predict(x_input)\n",
        "  pred_class = np.argmax(class_probs,axis=1)[0]\n",
        "  agression = list(task_a_label_dict.keys())[pred_class]\n",
        "  return agression, class_probs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGv2GX5O7XYi"
      },
      "source": [
        "## GUI input for sub-task A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "ebHVtAg24EHF",
        "outputId": "63bf1907-4cdf-44d6-d3d3-f0761242e68c"
      },
      "source": [
        "#@title Agression Level\n",
        "sentence = \"Aaami tomaake ekdum jore maarbo\" #@param {type:\"string\"}\n",
        "agr, class_probs = agression(sentence)\n",
        "print(\"Agression level = \",agr)\n",
        "print(\"NAG probability =\",class_probs[0])\n",
        "print(\"OAG probability =\",class_probs[1])\n",
        "print(\"CAG probability =\",class_probs[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agression level =  OAG\n",
            "NAG probability = 0.29471526\n",
            "OAG probability = 0.3587213\n",
            "CAG probability = 0.34656352\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}